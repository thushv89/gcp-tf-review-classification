{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b4fdb9",
   "metadata": {},
   "source": [
    "# Deploying an ML model on Vertex AI\n",
    "\n",
    "In this tutorial, you will be learning to deploy an ML model on Vertex AI. Specifically you will,\n",
    "\n",
    "* Download a text dataset and process that locally\n",
    "* Upload the processed data to GCS\n",
    "* Create a Vertex AI dataset\n",
    "* Create a custom ML model in TensorFlow and train it on the data\n",
    "* Deploy the model with an endpoint\n",
    "* Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da59a07b",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/gcp-tf-review-classification/blob/master/deploying_model_on_vertex_ai.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"/>Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c30a626",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Create a project called `gcp-tf-review-classification`\n",
    "* Enable Vertex AI API\n",
    "* Open up one tab for GCS and one for Vertex AI\n",
    "* Open this notebook in Google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c148ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapely<2 avoids the ContextualVersionConflict error by google-cloud-aiplatform\n",
    "!pip3 install --upgrade  pydantic google-cloud-aiplatform google-cloud-storage \"shapely<2\" tensorflow-text==2.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f96b26",
   "metadata": {},
   "source": [
    "## Print tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70438449",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f6b0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246eb19d",
   "metadata": {},
   "source": [
    "## Setting environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15960f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"gdsc-tensorflow-workshop\"\n",
    "BUCKET_URI = \"\" # e.g. gs://imdb-movie-review-dataset-thga\n",
    "MODEL_BUCKET_URI = \"\" # e.g. \"gs://imdb-movie-review-models-thga\" \n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81fb7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"BUCKET_URI\"] = BUCKET_URI\n",
    "os.environ[\"MODEL_BUCKET_URI\"] = MODEL_BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9baf52",
   "metadata": {},
   "source": [
    "## Authenticate to GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "# Initialize the Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f9634",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca792bd6",
   "metadata": {},
   "source": [
    "## Downloading the dataset\n",
    "\n",
    "Here we are using a sentiment classification dataset by Stanford. \n",
    "\n",
    "Read more: https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "def maybe_download(url: str, download_dir: str = \"data\"):\n",
    "    _, filename = os.path.split(url)\n",
    "    download_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    if not os.path.exists(download_path):\n",
    "        print(\"Downloading data\")\n",
    "        response = requests.get(url)\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "        with open(download_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(\"File is already downloaded\")\n",
    "\n",
    "    tar = tarfile.open(download_path, \"r:gz\")\n",
    "    tar.extractall(os.path.join(download_dir,\"unzip\"))\n",
    "    tar.close()\n",
    "\n",
    "maybe_download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c0dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_truncated_dataset(dataset_dir: str, n=2000):\n",
    "    \"\"\"It takes painstakenly long time to create a vertex AI dataset.\n",
    "    Therefore we're going to limit the data we'll be using\"\"\"\n",
    "\n",
    "    n_per_label = n//2\n",
    "    parent_dir, _ = os.path.split(dataset_dir)\n",
    "    truncated_dir = os.path.join(parent_dir, \"unzip_truncated\")\n",
    "    os.makedirs(truncated_dir, exist_ok=True)\n",
    "\n",
    "    for sub_dir in [\n",
    "        os.path.join(\"aclImdb\", \"train\", \"neg\"), \n",
    "        os.path.join(\"aclImdb\", \"train\", \"pos\"), \n",
    "        os.path.join(\"aclImdb\", \"test\", \"neg\"), \n",
    "        os.path.join(\"aclImdb\", \"test\", \"pos\")\n",
    "    ]:\n",
    "\n",
    "        os.makedirs(os.path.join(truncated_dir, sub_dir), exist_ok=True)\n",
    "        for f in os.listdir(os.path.join(dataset_dir, sub_dir)):\n",
    "            sample_id = int(f.split(\"_\")[0])\n",
    "            if sample_id < n_per_label:\n",
    "                shutil.copy(os.path.join(dataset_dir, sub_dir, f), os.path.join(truncated_dir, sub_dir, f))\n",
    "\n",
    "generate_truncated_dataset(os.path.join(\"data/unzip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3594f",
   "metadata": {},
   "source": [
    "## Create a GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if $(gsutil ls $BUCKET_URI); then\n",
    "    echo \"Bucket ${BUCKET_URI} already exists.\";\n",
    "else\n",
    "    echo \"Bucket ${BUCKET_URI} doesn't exist. Creating a new one\"\n",
    "    gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI\n",
    "fi\n",
    "\n",
    "if $(gsutil ls $MODEL_BUCKET_URI); then\n",
    "    echo \"Bucket ${MODEL_BUCKET_URI} already exists.\";\n",
    "else\n",
    "    echo \"Bucket ${MODEL_BUCKET_URI} doesn't exist. Creating a new one\"\n",
    "    gsutil mb -l $REGION -p $PROJECT_ID $MODEL_BUCKET_URI\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862a9d4",
   "metadata": {},
   "source": [
    "## Copy the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -m for parallel copying\n",
    "!gsutil -mq cp -r data/unzip_truncated/aclImdb/** $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8e01c",
   "metadata": {},
   "source": [
    "## Data on Vertex AI\n",
    "\n",
    "On Vertex AI, the data you use needs to be in a specfic format. Here's a guide to understanding [the process](https://cloud.google.com/vertex-ai/docs/text-data/classification/prepare-data). \n",
    "\n",
    "More specific information about instances of data: [here](https://cloud.google.com/vertex-ai/docs/reference/rpc/google.cloud.aiplatform.v1/schema#google.cloud.aiplatform.v1.schema.TextDataItem)\n",
    "\n",
    "## Parsing and dumping json with Pydantic\n",
    "\n",
    "add links and info about the schema we have to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24349bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic \n",
    "from typing import Dict,List,Literal\n",
    "\n",
    "class ClassificationAnnotation(pydantic.BaseModel):\n",
    "    displayName: Literal[\"positive\", \"negative\"]\n",
    "\n",
    "class DataItemResourceLabels(pydantic.BaseModel):\n",
    "    ml_use: Literal[\"training\", \"validation\", \"test\"] = pydantic.Field(alias=\"aiplatform.googleapis.com/ml_use\")\n",
    "    # Enables us to use ml_use=<x> instead of the long field name\n",
    "    class Config:\n",
    "        allow_population_by_field_name = True\n",
    "\n",
    "class TextClassificationSample(pydantic.BaseModel):\n",
    "    textContent: str\n",
    "    classificationAnnotation: ClassificationAnnotation \n",
    "    dataItemResourceLabels: DataItemResourceLabels\n",
    "\n",
    "instance = TextClassificationSample(\n",
    "    textContent=\"some review text\", \n",
    "    classificationAnnotation=ClassificationAnnotation(displayName=\"positive\"),\n",
    "    dataItemResourceLabels=DataItemResourceLabels(ml_use=\"training\")\n",
    ")\n",
    "\n",
    "print(instance.json(by_alias=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90269925",
   "metadata": {},
   "source": [
    "## Creating Vertex AI compatible instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ce8d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage \n",
    "random.seed(946021)\n",
    "\n",
    "def read_gcs_with_full_path(storage_client, bucket_name, blob_name):\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    with blob.open(\"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_single_instance(bucket_name, blob_name, ml_use, storage_client):\n",
    "\n",
    "    label = None\n",
    "    if blob_name.endswith(\".txt\"):\n",
    "        if \"pos\" in blob_name:\n",
    "            label = \"positive\"\n",
    "        elif \"neg\" in blob_name:\n",
    "            label = \"negative\"\n",
    "        if label:\n",
    "            instance = TextClassificationSample(\n",
    "                textContent=read_gcs_with_full_path(\n",
    "                    storage_client=storage_client,\n",
    "                    bucket_name=bucket_name,\n",
    "                    blob_name=blob_name, \n",
    "                ),\n",
    "                classificationAnnotation=ClassificationAnnotation(displayName=label),\n",
    "                dataItemResourceLabels=DataItemResourceLabels(ml_use=ml_use)\n",
    "            )\n",
    "            return instance\n",
    "    return None\n",
    "\n",
    "def create_instances(bucket_uri):\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    train_gcs_bucket_prefix = \"train\"\n",
    "    test_gcs_bucket_prefix = \"test\"\n",
    "    bucket_name = bucket_uri[5:]\n",
    "\n",
    "    train_instances = []\n",
    "\n",
    "    # delimiter only return the items in that directory (exclude subdirs)\n",
    "    train_blobs = storage_client.list_blobs(bucket_name, prefix=train_gcs_bucket_prefix)\n",
    "  \n",
    "    print(f\"Reading training data from the GCS bucket\")\n",
    "    for b in train_blobs:\n",
    "        instance = generate_single_instance(\n",
    "            bucket_name=bucket_name, blob_name=b.name, ml_use=\"training\", storage_client=storage_client\n",
    "        )\n",
    "        if instance:\n",
    "            train_instances.append(instance.json(by_alias=True)+'\\n')\n",
    "    print(f\"\\tFound {len(train_instances)} train instances\")\n",
    "\n",
    "    test_instances = []\n",
    "    valid_count, test_count = 0,0\n",
    "\n",
    "    test_blobs = storage_client.list_blobs(bucket_name, prefix=test_gcs_bucket_prefix)\n",
    "    print(f\"Reading test data from the GCS bucket\")\n",
    "    for b in test_blobs:\n",
    "        if random.uniform(0,1.0)<0.5:\n",
    "            valid_count += 1\n",
    "            ml_use=\"validation\"\n",
    "        else:\n",
    "            test_count += 1\n",
    "            ml_use=\"test\"\n",
    "\n",
    "        instance = generate_single_instance(\n",
    "            bucket_name=bucket_name, blob_name=b.name, ml_use=ml_use, storage_client=storage_client\n",
    "        )\n",
    "        if instance: \n",
    "            test_instances.append(instance.json(by_alias=True)+'\\n')\n",
    "\n",
    "    print(f\"\\tFound {valid_count} validation instances and {test_count} test instances\")\n",
    "\n",
    "    return train_instances, test_instances\n",
    "\n",
    "train_instances, test_instances = create_instances(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf12710",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_instances[:1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8aee51a",
   "metadata": {},
   "source": [
    "## Writing the data to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(\"data\", \"train_instances.jsonl\"), \"w\") as f:\n",
    "    f.writelines(train_instances)\n",
    "\n",
    "with open(os.path.join(\"data\", \"test_instances.jsonl\"), \"w\") as f:\n",
    "    f.writelines(test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f170635",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp data/train_instances.jsonl $BUCKET_URI\n",
    "!gsutil cp data/test_instances.jsonl $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a809a",
   "metadata": {},
   "source": [
    "## Vertex AI specific constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"imdb-review-dataset\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "901f2e4c",
   "metadata": {},
   "source": [
    "## Creating a Vertex AI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c195f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_uris = [os.path.join(BUCKET_URI,f\"{subset}_instances.jsonl\") for subset in [\"train\", \"test\"]]\n",
    "\n",
    "datasets = aiplatform.TextDataset.list()\n",
    "dataset = None\n",
    "dataset_exists = False\n",
    "if len(datasets)>0:\n",
    "    for ds in datasets:\n",
    "        if ds.display_name == DATASET_NAME:\n",
    "            print(f\"Existing dataset found {ds.display_name} at {ds.gca_resource.name}\")\n",
    "            dataset = ds\n",
    "            dataset_exists = True\n",
    "            break\n",
    "    \n",
    "if not dataset_exists:\n",
    "    print(f\"Dataset with name {DATASET_NAME} was not found creating one...\")\n",
    "    dataset = aiplatform.TextDataset.create(\n",
    "        display_name=DATASET_NAME, \n",
    "        gcs_source=schema_uris, \n",
    "        import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fba9e5",
   "metadata": {},
   "source": [
    "## Training a model \n",
    "\n",
    "There are two methods for getting a model into Vertex AI\n",
    "\n",
    "1. Run a training job on Vertex AI (**This can take several hours to complete**)\n",
    "  * AutoMLTextTrainingJob\n",
    "  * CustomMLTrianingJob\n",
    "2. Upload an already trained model to Vertex AI  \n",
    " \n",
    "\n",
    "### Quotas to know for custom models training\n",
    "\n",
    "```\n",
    "custom_model_training_cpus\n",
    "custom_model_training_n2_cpus`\n",
    "all_regions_gpus\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3f8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOML_JOB_NAME = \"\" # e.g. \"imdb_review_classification_automl_train_thga\"\n",
    "\n",
    "AUTOML_MODEL_ID = \"\" # e.g. \"imdb-review-model-thga\"\n",
    "AUTOML_MODEL_VERSION_ALIAS = \"automl\"\n",
    "\n",
    "\n",
    "training_job = aiplatform.AutoMLTextTrainingJob(\n",
    "    display_name=AUTOML_JOB_NAME,\n",
    "    prediction_type=\"classification\",\n",
    ")\n",
    "\n",
    "# Use training_job.state to poll the current state of the job\n",
    "model = training_job.run(\n",
    "    dataset=dataset,\n",
    "    training_filter_split=\"labels.aiplatform.googleapis.com/ml_use=training\",\n",
    "    validation_filter_split=\"labels.aiplatform.googleapis.com/ml_use=validation\",\n",
    "    test_filter_split=\"labels.aiplatform.googleapis.com/ml_use=test\",\n",
    "    model_id=AUTOML_MODEL_ID,\n",
    "    model_version_aliases=[AUTOML_MODEL_VERSION_ALIAS],\n",
    "    model_version_description=\"Initial version of the automl model\",\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c7117d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Before you proceed any further, make sure you have uploaded the trained model successfully to the `MODEL_BUCKET_URI` (`training_custom_model.ipynb`)\n",
    "---\n",
    "\n",
    "## Uploading the trained model\n",
    "\n",
    "**This is a long running operation**. To monitor this process, you can simply use gcloud, e.g.\n",
    "\n",
    "```\n",
    "curl -X GET -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \"https://us-central1-aiplatform.googleapis.com/v1/projects/<project_id>/locations/us-central1/models/<model_id>/operations/<operation_id>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24165f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_ID = \"\" #e.g. \"imdb-review-model-thga\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f63724",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai models upload \\\n",
    "  --region=$REGION \\\n",
    "  --model-id=$CUSTOM_MODEL_ID \\\n",
    "  --display-name=\"imdb-review-model\" \\\n",
    "  --container-image-uri=\"us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-cpu.2-11:latest\" \\\n",
    "  --artifact-uri=$MODEL_BUCKET_URI/text_classifier \\\n",
    "  --version-aliases=\"custom\" \\\n",
    "  --verbosity=\"info\"\n",
    "\n",
    "#aiplatform.Model.upload(\n",
    "#    model_id=CUSTOM_MODEL_ID,\n",
    "#    display_name=CUSTOM_MODEL_ID,\n",
    "#    serving_container_image_uri=\"tensorflow/serving:2.11.0\",\n",
    "#    artifact_uri=MODEL_BUCKET_URI+\"/text_classifier\",\n",
    "#    version_aliases=[\"custom\"],\n",
    "#    sync=False\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a3bccb",
   "metadata": {},
   "source": [
    "## Deploying a model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3e4051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got endpoint ID: 6304120199\n"
     ]
    }
   ],
   "source": [
    "def get_random_endpoint_id(name, seed=1234):\n",
    "    random.seed(seed)\n",
    "    return abs(hash(name))%10000000000\n",
    "\n",
    "endpoint_id = get_random_endpoint_id(\"thushan ganegedara\")\n",
    "print(f\"Got endpoint ID: {endpoint_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_ID = \"\" #.e.g \"123456\"\n",
    "endpoint = aiplatform.Endpoint(endpoint_name=ENDPOINT_ID)\n",
    "endpoint.create(display_name=\"imdb-review-predict\", endpoint_id=ENDPOINT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd351c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resource_name = MODEL_ID + \"@1\" \n",
    "model = aiplatform.Model(model_name=MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fcb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model found: {model.versioned_resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67df69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(traffic_percentage=100, min_replica_count=1, max_replica_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b672114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the endpoint if you don't have the reference\n",
    "endpoint = aiplatform.Endpoint(endpoint_name=ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb221309",
   "metadata": {},
   "source": [
    "## Predicting using the deployed endpoint (AutoML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78487eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get format\n",
    "# Go to the model on the dashboard and \"Deploy & Test\"\n",
    "# Open the inspection console and go to network\n",
    "# Type in an input and click \"Predict\"\n",
    "# Click on the relevant netowkr call and click \"Request\"\n",
    "\n",
    "# Click sample request in the Endpoint to understand how to call it\n",
    "input_raw = {\"content\":\"horrible movie, it was so predictable\"}\n",
    "\n",
    "predictions = endpoint.predict(instances=[input_raw])\n",
    "y_class = np.argmax(predictions.predictions[0][\"confidences\"])\n",
    "y_label_name = predictions.predictions[0][\"displayNames\"][y_class]\n",
    "\n",
    "print(f\"Input text: {input_raw['content']}\")\n",
    "print(f\"\\tLabel: {y_label_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5442c83e",
   "metadata": {},
   "source": [
    "## Predicting using the deployed endpoint (Custom model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66a00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_raw = [\"horrible movie, it was so predictable\"]\n",
    "\n",
    "predictions = endpoint.predict(instances=inputs_raw)\n",
    "\n",
    "label_map = {0: \"negative\", 1:\"positive\"}\n",
    "for inp, pred in zip(inputs_raw, predictions):\n",
    "    y_class = predictions.predictions[0][0] > 0.5\n",
    "    y_label_name = label_map[y_class]\n",
    "\n",
    "    print(f\"Input text: {inp}\")\n",
    "    print(f\"\\tLabel: {y_label_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22316bee",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba2c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Delete the training job\n",
    "print(\"Deleting the training job\")\n",
    "training_job = aiplatform.AutoMLTextTrainingJob.get(\n",
    "    resource_name=\"\",\n",
    ")\n",
    "training_job.delete()\n",
    "\n",
    "\n",
    "# Delete the model\n",
    "print(\"Deleting the model\")\n",
    "model = aiplatform.Model(model_name=MODEL_ID)\n",
    "model.delete()\n",
    "print(f\"The model {MODEL_ID} was successfully deleted.\\n\")\n",
    "\n",
    "# Delete the endpoint\n",
    "print(\"Deleting the endpoint\")\n",
    "endpoint = aiplatform.Endpoint(endpoint_name=ENDPOINT_NAME)\n",
    "endpoint.delete()\n",
    "print(f\"The endpoint {ENDPOINT_NAME} was sucessfully deleted.\\n\")\n",
    "\n",
    "# Warning: Setting this to true deletes everything in your bucket\n",
    "delete_bucket = False\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd0529e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
