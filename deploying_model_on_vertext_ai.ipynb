{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80569b65",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/gcp-tf-review-classification/blob/master/deploying_model_on_vertex_ai.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"/>Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a822bf18",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Create a project called `gcp-tf-review-classification`\n",
    "\n",
    "Open this notebook in Google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5088b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade  pydantic google-cloud-aiplatform google-cloud-storage \"shapely<2\" tensorflow-text==2.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c9cce",
   "metadata": {},
   "source": [
    "## Print tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95ee30",
   "metadata": {},
   "source": [
    "## Authenticate to GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a850b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa9058b",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535e054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14ad21",
   "metadata": {},
   "source": [
    "## Setting environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8678dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"gcp-tf-review-classification\"\n",
    "BUCKET_URI = \"gs://imdb-movie-review-dataset\" \n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002653ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"BUCKET_URI\"] = BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd15fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd72b3d3",
   "metadata": {},
   "source": [
    "## Downloading the dataset\n",
    "\n",
    "Here we are using a sentiment classification dataset by Stanford. \n",
    "\n",
    "Read more: https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8674c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "def maybe_download(url: str, download_dir: str = \"data\"):\n",
    "    _, filename = os.path.split(url)\n",
    "    download_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    if not os.path.exists(download_path):\n",
    "        print(\"Downloading data\")\n",
    "        response = requests.get(url)\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "        with open(download_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(\"File is already downloaded\")\n",
    "\n",
    "    tar = tarfile.open(download_path, \"r:gz\")\n",
    "    tar.extractall(os.path.join(download_dir,\"unzip\"))\n",
    "    tar.close()\n",
    "\n",
    "maybe_download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_truncated_dataset(dataset_dir: str, n=2000):\n",
    "    \"\"\"It takes painstakenly long time to create a vertex AI dataset.\n",
    "    Therefore we're going to limit the data we'll be using\"\"\"\n",
    "\n",
    "    n_per_label = n//2\n",
    "    parent_dir, _ = os.path.split(dataset_dir)\n",
    "    truncated_dir = os.path.join(parent_dir, \"unzip_truncated\")\n",
    "    os.makedirs(truncated_dir, exist_ok=True)\n",
    "\n",
    "    for sub_dir in [\n",
    "        os.path.join(\"aclImdb\", \"train\", \"neg\"), \n",
    "        os.path.join(\"aclImdb\", \"train\", \"pos\"), \n",
    "        os.path.join(\"aclImdb\", \"test\", \"neg\"), \n",
    "        os.path.join(\"aclImdb\", \"test\", \"pos\")\n",
    "    ]:\n",
    "\n",
    "        os.makedirs(os.path.join(truncated_dir, sub_dir), exist_ok=True)\n",
    "        for f in os.listdir(os.path.join(dataset_dir, sub_dir)):\n",
    "        sample_id = int(f.split(\"_\")[0])\n",
    "        if sample_id < n_per_label:\n",
    "            shutil.copy(os.path.join(dataset_dir, sub_dir, f), os.path.join(truncated_dir, sub_dir, f))\n",
    "\n",
    "generate_truncated_dataset(os.path.join(\"data/unzip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d310faf2",
   "metadata": {},
   "source": [
    "## Create a GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b51645",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if $(gsutil ls ${BUCKET_URI}); then  \n",
    "    echo \"Bucket ${BUCKET_URI} already exists.\"\n",
    "else\n",
    "    echo \"Bucket ${BUCKET_URI} doesn't exist. Creating a new one\"\n",
    "    gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19fae5",
   "metadata": {},
   "source": [
    "## Copy the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d437d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -m for parallel copying\n",
    "!gsutil -mq cp -r data/unzip_truncated/aclImdb/** $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923fe4ce",
   "metadata": {},
   "source": [
    "## Managing json Schema with Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9016ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic \n",
    "from typing import Dict,List,Literal\n",
    "\n",
    "class ClassificationAnnotation(pydantic.BaseModel):\n",
    "    displayName: Literal[\"positive\", \"negative\"]\n",
    "\n",
    "class DataItemResourceLabels(pydantic.BaseModel):\n",
    "    ml_use: Literal[\"training\", \"validation\", \"test\"] = pydantic.Field(alias=\"aiplatform.googleapis.com/ml_use\")\n",
    "    # Enables us to use ml_use=<x> instead of the long field name\n",
    "    class Config:\n",
    "        allow_population_by_field_name = True\n",
    "\n",
    "class TextClassificationSample(pydantic.BaseModel):\n",
    "    textContent: str\n",
    "    classificationAnnotation: ClassificationAnnotation \n",
    "    dataItemResourceLabels: DataItemResourceLabels\n",
    "\n",
    "instance = TextClassificationSample(\n",
    "    textContent=\"some review text\", \n",
    "    classificationAnnotation=ClassificationAnnotation(displayName=\"positive\"),\n",
    "    dataItemResourceLabels=DataItemResourceLabels(ml_use=\"training\")\n",
    ")\n",
    "\n",
    "print(instance.json(by_alias=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe19ad6",
   "metadata": {},
   "source": [
    "## Creating Vertex AI compatible instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f6cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage \n",
    "random.seed(946021)\n",
    "\n",
    "# TODO: rename to read_from_gcs\n",
    "def read_gcs_with_full_path(storage_client, bucket_name, blob_name):\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    with blob.open(\"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_single_instance(bucket_name, blob_name, ml_use, storage_client):\n",
    "\n",
    "    label = None\n",
    "    if blob_name.endswith(\".txt\"):\n",
    "        if \"pos\" in blob_name:\n",
    "            label = \"positive\"\n",
    "        elif \"neg\" in blob_name:\n",
    "            label = \"negative\"\n",
    "        if label:\n",
    "            instance = TextClassificationSample(\n",
    "                textContent=read_gcs_with_full_path(\n",
    "                    storage_client=storage_client,\n",
    "                    bucket_name=bucket_name,\n",
    "                    blob_name=blob_name, \n",
    "                ),\n",
    "                classificationAnnotation=ClassificationAnnotation(displayName=label),\n",
    "                dataItemResourceLabels=DataItemResourceLabels(ml_use=ml_use)\n",
    "            )\n",
    "            return instance\n",
    "    return None\n",
    "\n",
    "def create_instances(bucket_uri):\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    train_gcs_bucket_prefix = \"train\"\n",
    "    test_gcs_bucket_prefix = \"test\"\n",
    "    bucket_name = bucket_uri[5:]\n",
    "\n",
    "    train_instances = []\n",
    "\n",
    "    # delimiter only return the items in that directory (exclude subdirs)\n",
    "    train_blobs = storage_client.list_blobs(bucket_name, prefix=train_gcs_bucket_prefix)\n",
    "  \n",
    "    print(f\"Reading training data from the GCS bucket\")\n",
    "    for b in train_blobs:\n",
    "        instance = generate_single_instance(\n",
    "            bucket_name=bucket_name, blob_name=b.name, ml_use=\"training\", storage_client=storage_client\n",
    "        )\n",
    "        if instance:\n",
    "            train_instances.append(instance.json(by_alias=True)+'\\n')\n",
    "    print(f\"\\tFound {len(train_instances)} train instances\")\n",
    "\n",
    "    test_instances = []\n",
    "    valid_count, test_count = 0,0\n",
    "\n",
    "    test_blobs = storage_client.list_blobs(bucket_name, prefix=test_gcs_bucket_prefix)\n",
    "    print(f\"Reading test data from the GCS bucket\")\n",
    "    for b in test_blobs:\n",
    "        if random.uniform(0,1.0)<0.5:\n",
    "            valid_count += 1\n",
    "            ml_use=\"validation\"\n",
    "        else:\n",
    "            test_count += 1\n",
    "            ml_use=\"test\"\n",
    "\n",
    "        instance = generate_single_instance(\n",
    "            bucket_name=bucket_name, blob_name=b.name, ml_use=ml_use, storage_client=storage_client\n",
    "        )\n",
    "        if instance: \n",
    "            test_instances.append(instance.json(by_alias=True)+'\\n')\n",
    "\n",
    "    print(f\"\\tFound {valid_count} validation instances and {test_count} test instances\")\n",
    "\n",
    "    return train_instances, test_instances\n",
    "\n",
    "train_instances, test_instances = create_instances(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7673bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_instances[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f633f41",
   "metadata": {},
   "source": [
    "## Writing the data to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94dba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(\"data\", \"train_instances.jsonl\"), \"w\") as f:\n",
    "    f.writelines(train_instances)\n",
    "\n",
    "with open(os.path.join(\"data\", \"test_instances.jsonl\"), \"w\") as f:\n",
    "    f.writelines(test_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe987082",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp data/train_instances.jsonl $BUCKET_URI\n",
    "!gsutil cp data/test_instances.jsonl $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde0f4f",
   "metadata": {},
   "source": [
    "## Creating a Vertex AI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292640da",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"imdb-review-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62370b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52cbd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_uris = [os.path.join(BUCKET_URI,f\"{subset}_instances.jsonl\") for subset in [\"train\", \"test\"]]\n",
    "\n",
    "\n",
    "datasets = aiplatform.TextDataset.list()\n",
    "dataset = None\n",
    "if len(datasets)>0:\n",
    "    for ds in datasets:\n",
    "        if ds.display_name == DATASET_NAME:\n",
    "            print(f\"Existing dataset found {ds.display_name} at {ds.gca_resource.name}\")\n",
    "            dataset = ds\n",
    "            break\n",
    "        else:\n",
    "        print(f\"Dataset with name {DATASET_NAME} was not found creating one...\")\n",
    "        dataset = aiplatform.TextDataset.create(\n",
    "            display_name=DATASET_NAME, \n",
    "            gcs_source=schema_uris, \n",
    "            import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e0d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = \"imdb_review_classification_automl_train\"\n",
    "\n",
    "MODEL_ID = \"imdb-review-model\"\n",
    "MODEL_VERSION_ALIAS = \"v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18a379",
   "metadata": {},
   "source": [
    "## This can take several hours to complete\n",
    "\n",
    "## Quotas for custom models\n",
    "`custom_model_training_cpus`\n",
    "`custom_model_training_n2_cpus`\n",
    "`all_regions_gpus`\n",
    "\n",
    "\n",
    "```\n",
    "training_job = aiplatform.AutoMLTextTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    prediction_type=\"classification\",\n",
    ")\n",
    "\n",
    "# Use training_job.state to poll the current state of the job\n",
    "model = training_job.run(\n",
    "    dataset=dataset,\n",
    "    training_filter_split=\"labels.aiplatform.googleapis.com/ml_use=training\",\n",
    "    validation_filter_split=\"labels.aiplatform.googleapis.com/ml_use=validation\",\n",
    "    test_filter_split=\"labels.aiplatform.googleapis.com/ml_use=test\",\n",
    "    model_id=MODEL_ID,\n",
    "    model_version_aliases=[MODEL_VERSION_ALIAS],\n",
    "    model_version_description=\"Initial version of the model\",\n",
    "    sync=False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533850c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud ai models upload \\\n",
    "  --region=LOCATION \\\n",
    "  --display-name=MODEL_NAME \\\n",
    "  --container-image-uri=IMAGE_URI \\\n",
    "  --artifact-uri=PATH_TO_MODEL_ARTIFACT_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4eb9b3",
   "metadata": {},
   "source": [
    "## Referring to a model async\n",
    "\n",
    "\n",
    "```\n",
    "model_resource_name = MODEL_ID#+\"@\"+MODEL_VERSION_ALIAS\n",
    "model = aiplatform.Model(model_name=MODEL_ID)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff58e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model found: {model.versioned_resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ec451",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = model.deploy(traffic_percentage=100, min_replica_count=1, max_replica_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f4207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the endpoint if you don't have the reference\n",
    "endpoint = aiplatform.Endpoint(endpoint_name=ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get format\n",
    "# Go to the model on the dashboard and \"Deploy & Test\"\n",
    "# Open the inspection console and go to network\n",
    "# Type in an input and click \"Predict\"\n",
    "# Click on the relevant netowkr call and click \"Request\"\n",
    "\n",
    "# Click sample request in the Endpoint to understand how to call it\n",
    "input_raw = {\"content\":\"horrible movie, it was so predictable\"}\n",
    "\n",
    "predictions = endpoint.predict(instances=[input_raw])\n",
    "y_class = np.argmax(predictions.predictions[0][\"confidences\"])\n",
    "y_label_name = predictions.predictions[0][\"displayNames\"][y_class]\n",
    "\n",
    "print(f\"Input text: {input_raw['content']}\")\n",
    "print(f\"\\tLabel: {y_label_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4573b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_raw = {\"content\":\"horrible movie, it was so predictable\"}\n",
    "\n",
    "predictions = endpoint.predict(instances=[input_raw])\n",
    "y_class = np.argmax(predictions.predictions[0][\"confidences\"])\n",
    "y_label_name = predictions.predictions[0][\"displayNames\"][y_class]\n",
    "\n",
    "print(f\"Input text: {input_raw['content']}\")\n",
    "print(f\"\\tLabel: {y_label_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74648794",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33246d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Delete the training job\n",
    "print(\"Deleting the training job\")\n",
    "training_job = aiplatform.AutoMLTextTrainingJob.get(\n",
    "    resource_name=\"2588955296177061888\",\n",
    ")\n",
    "training_job.delete()\n",
    "#print(f\"Training job {JOB_NAME} was successfully deleted.\\n\")\n",
    "\n",
    "# Delete the model\n",
    "print(\"Deleting the model\")\n",
    "model = aiplatform.Model(model_name=MODEL_ID)\n",
    "model.delete()\n",
    "print(f\"The model {MODEL_ID} was successfully deleted.\\n\")\n",
    "\n",
    "# Delete the endpoint\n",
    "print(\"Deleting the endpoint\")\n",
    "endpoint = aiplatform.Endpoint(endpoint_name=ENDPOINT_NAME)\n",
    "endpoint.delete()\n",
    "print(f\"The endpoint {ENDPOINT_NAME} was sucessfully deleted.\\n\")\n",
    "\n",
    "# Warning: Setting this to true deletes everything in your bucket\n",
    "delete_bucket = False\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ce16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_txt_dict = preprocessor([datasets[\"training\"][\"inputs\"][0]])\n",
    "out = encoder(preproc_txt_dict)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
