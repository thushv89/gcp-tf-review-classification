{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f9e07f",
   "metadata": {},
   "source": [
    "# Creating and training a custom model\n",
    "\n",
    "In this section of the tutorial you will,\n",
    "\n",
    "* Download the data from GCS and process to be appropriate for training a model\n",
    "* Create a model which uses BERT as the base\n",
    "* Train the model on the processed data\n",
    "* Save the model and upload to GCS\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/gcp-tf-review-classification/blob/master/training_custom_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"/>Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f75dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade  pydantic google-cloud-aiplatform google-cloud-storage \"shapely<2\" tensorflow-text==2.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe32b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0559308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"gdsc-tensorflow-workshop\"\n",
    "BUCKET_URI = \"\" # e.g. gs://imdb-movie-review-dataset-thga\n",
    "MODEL_BUCKET_URI = \"\" # e.g. \"gs://imdb-movie-review-models-thga\" \n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"BUCKET_URI\"] = BUCKET_URI\n",
    "os.environ[\"MODEL_BUCKET_URI\"] = MODEL_BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b1545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b52a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "DATASET_NAME = \"imdb-review-dataset\"\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbbce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = aiplatform.TextDataset.list()\n",
    "\n",
    "DATASET_NAME = \"imdb-review-dataset\"\n",
    "\n",
    "DATASET_RESOURCE_NAME = \"\"\n",
    "for dataset in datasets:\n",
    "    if dataset.display_name == DATASET_NAME:\n",
    "        DATASET_RESOURCE_NAME = dataset.resource_name\n",
    "        break\n",
    "print(f\"Dataset resource name: {DATASET_RESOURCE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c041a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aiplatform.TextDataset(aiplatform.TextDataset.list()[0].resource_name)\n",
    "\n",
    "# Automatically creates a directory with the name exported_data_<datetime> - no need to provide\n",
    "exported_files = dataset.export_data(output_dir=BUCKET_URI)\n",
    "\n",
    "print(\"Following files were exported\")\n",
    "print(exported_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydantic \n",
    "from typing import Any, Dict,List,Literal\n",
    "\n",
    "class ClassificationAnnotation(pydantic.BaseModel):\n",
    "    displayName: Literal[\"positive\", \"negative\"]\n",
    "\n",
    "class DataItemResourceLabels(pydantic.BaseModel):\n",
    "    ml_use: Literal[\"training\", \"validation\", \"test\"] = pydantic.Field(alias=\"aiplatform.googleapis.com/ml_use\")\n",
    "    # Enables us to use ml_use=<x> instead of the long field name\n",
    "    class Config:\n",
    "        allow_population_by_field_name = True\n",
    "\n",
    "class TextClassificationSample(pydantic.BaseModel):\n",
    "    textContent: str\n",
    "    classificationAnnotation: ClassificationAnnotation \n",
    "    dataItemResourceLabels: DataItemResourceLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from google.cloud import storage \n",
    "random.seed(946021)\n",
    "\n",
    "# TODO: rename to read_from_gcs\n",
    "def read_gcs_with_full_path(storage_client, bucket_name, blob_name):\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    with blob.open(\"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_single_instance(bucket_name, blob_name, ml_use, storage_client):\n",
    "\n",
    "    label = None\n",
    "    if blob_name.endswith(\".txt\"):\n",
    "        if \"pos\" in blob_name:\n",
    "            label = \"positive\"\n",
    "        elif \"neg\" in blob_name:\n",
    "            label = \"negative\"\n",
    "        if label:\n",
    "            instance = TextClassificationSample(\n",
    "                textContent=read_gcs_with_full_path(\n",
    "                    storage_client=storage_client,\n",
    "                    bucket_name=bucket_name,\n",
    "                    blob_name=blob_name, \n",
    "                ),\n",
    "                classificationAnnotation=ClassificationAnnotation(displayName=label),\n",
    "                dataItemResourceLabels=DataItemResourceLabels(ml_use=ml_use)\n",
    "            )\n",
    "            return instance\n",
    "    return None\n",
    "\n",
    "def create_instances(bucket_uri):\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    train_gcs_bucket_prefix = \"train\"\n",
    "    test_gcs_bucket_prefix = \"test\"\n",
    "    bucket_name = bucket_uri[5:]\n",
    "\n",
    "    train_instances = []\n",
    "\n",
    "    # delimiter only return the items in that directory (exclude subdirs)\n",
    "    train_blobs = storage_client.list_blobs(bucket_name, prefix=train_gcs_bucket_prefix)\n",
    "  \n",
    "    print(f\"Reading training data from the GCS bucket\")\n",
    "    for b in train_blobs:\n",
    "        instance = generate_single_instance(\n",
    "            bucket_name=bucket_name, blob_name=b.name, ml_use=\"training\", storage_client=storage_client\n",
    "        )\n",
    "        if instance:\n",
    "            train_instances.append(instance)\n",
    "    print(f\"\\tFound {len(train_instances)} train instances\")\n",
    "\n",
    "    test_instances = []\n",
    "    valid_count, test_count = 0,0\n",
    "\n",
    "    test_blobs = storage_client.list_blobs(bucket_name, prefix=test_gcs_bucket_prefix)\n",
    "    print(f\"Reading test data from the GCS bucket\")\n",
    "    for b in test_blobs:\n",
    "        if random.uniform(0,1.0)<0.5:\n",
    "            valid_count += 1\n",
    "            ml_use=\"validation\"\n",
    "        else:\n",
    "            test_count += 1\n",
    "            ml_use=\"test\"\n",
    "\n",
    "        instance = generate_single_instance(\n",
    "            bucket_name=bucket_name, blob_name=b.name, ml_use=ml_use, storage_client=storage_client\n",
    "        )\n",
    "        if instance: \n",
    "            test_instances.append(instance)\n",
    "\n",
    "    print(f\"\\tFound {valid_count} validation instances and {test_count} test instances\")\n",
    "\n",
    "    instances = train_instances + test_instances\n",
    "    datasets = {\"training\":{\"inputs\":[], \"labels\": []}, \"validation\":{\"inputs\":[], \"labels\": []}, \"test\":{\"inputs\":[], \"labels\": []}}\n",
    "    label_map = {\"positive\": 1, \"negative\": 0}\n",
    "    for ins in instances:\n",
    "        datasets[ins.dataItemResourceLabels.ml_use][\"inputs\"].append(ins.textContent)\n",
    "        datasets[ins.dataItemResourceLabels.ml_use][\"labels\"].append(label_map[ins.classificationAnnotation.displayName])\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "datasets = create_instances(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeefca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"training\"][\"inputs\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e7c9dd",
   "metadata": {},
   "source": [
    "## Downloading the base model from TFHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf \n",
    "# Unless this import is here, the following error comes up\n",
    "# Error Op type not registered 'CaseFoldUTF8' in binary running on 932fd13e3432. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n",
    "# You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.\n",
    "import tensorflow_text\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def download_base_model() -> tf.keras.Model:\n",
    "    preprocessor = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    "    )\n",
    "    encoder = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/2\",\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    return preprocessor, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d33fa8",
   "metadata": {},
   "source": [
    "## Creating the full TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(preprocessor: hub.KerasLayer, encoder: hub.KerasLayer) -> tf.keras.Model:\n",
    "    \"\"\" Use the pretrained base and mount a head for sentiment analysis \"\"\"\n",
    "\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "\n",
    "    encoder_inputs = preprocessor(text_input)\n",
    "\n",
    "    encoder_outputs = encoder(encoder_inputs)\n",
    "\n",
    "    pooled_output = encoder_outputs[\"pooled_output\"]      # [batch_size, 128].\n",
    "    hidden_layer = tf.keras.layers.Dense(256, activation=\"gelu\")\n",
    "    classif_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    hidden_out = hidden_layer(pooled_output)\n",
    "    final_out = classif_layer(hidden_out)\n",
    "\n",
    "    model = tf.keras.Model(inputs=text_input, outputs=final_out)\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\", \n",
    "        optimizer=tf.keras.optimizers.Adam(), \n",
    "        metrics=\"accuracy\"\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac885e3",
   "metadata": {},
   "source": [
    "## Create a TensorFlow dataset to train and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tf_dataset(datasets: Dict[str, Any], subset: str, batch_size: int=128, shuffle:bool = False) -> tf.data.Dataset:\n",
    "    \"\"\" Create a tf.data.Dataset from the given data subset \"\"\"\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((datasets[subset][\"inputs\"], datasets[subset][\"labels\"]))\n",
    "    dataset = dataset.shuffle(batch_size*10) if shuffle else dataset \n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b2972",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b41560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "# Generate datasets\n",
    "train_ds = generate_tf_dataset(datasets, \"training\", batch_size=batch_size, shuffle=True)\n",
    "valid_ds = generate_tf_dataset(datasets, \"validation\", batch_size=batch_size)\n",
    "test_ds = generate_tf_dataset(datasets, \"test\", batch_size=batch_size)\n",
    "\n",
    "# Create the model\n",
    "preprocessor, encoder = download_base_model() \n",
    "model = create_model(preprocessor, encoder)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_ds, epochs=epochs, validation_data=valid_ds)\n",
    "\n",
    "# Save the model\n",
    "tf.saved_model.save(model, \"./text_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada50b0",
   "metadata": {},
   "source": [
    "## Upload the model to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77488b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r ./text_classifier $MODEL_BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
