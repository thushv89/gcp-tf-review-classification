{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece8896",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade  pydantic google-cloud-aiplatform google-cloud-storage \"shapely<2\" tensorflow-text==2.9.0\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a91a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_files = dataset.export_data(output_dir=BUCKET_URI)\n",
    "print(\"Following files were exported\")\n",
    "print(exported_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage \n",
    "\n",
    "def read_gcs_with_full_path(storage_client, filepath):\n",
    "\n",
    "  bucket, blob_name = filepath.split(\"/\")[2], \"/\".join(filepath.split(\"/\")[3:])\n",
    "  bucket = storage_client.bucket(bucket)\n",
    "  blob = bucket.blob(blob_name)\n",
    "\n",
    "  with blob.open(\"r\") as f:\n",
    "      data = f.read()\n",
    "\n",
    "  return data\n",
    "\n",
    "def read_gcs_from_exported_dataset(exported_files: List[str]):\n",
    "\n",
    "  storage_client = storage.Client()\n",
    "\n",
    "  for filepath in exported_files:\n",
    "\n",
    "    print(\"Reading the JSON line file\")\n",
    "    data = read_gcs_with_full_path(storage_client, filepath)\n",
    "\n",
    "    print(\"Building instances\")\n",
    "    instances = [ TextClassificationSample.parse_raw(ins) for ins in data.split(\"\\n\")]\n",
    "\n",
    "    print(\"Building data subsets\")\n",
    "    datasets = {\"training\":{\"inputs\":[], \"labels\": []}, \"validation\":{\"inputs\":[], \"labels\": []}, \"test\":{\"inputs\":[], \"labels\": []}}\n",
    "    label_map = {\"positive\": 1, \"negative\": 0}\n",
    "    for ins in instances:\n",
    "      txt = read_gcs_with_full_path(storage_client, ins.textGcsUri)\n",
    "      datasets[ins.dataItemResourceLabels.ml_use][\"inputs\"].append(txt)\n",
    "      datasets[ins.dataItemResourceLabels.ml_use][\"labels\"].append(label_map[ins.classificationAnnotation.displayName])\n",
    "\n",
    "  return datasets\n",
    "\n",
    "datasets = read_gcs_from_exported_dataset(exported_files)\n",
    "\n",
    "for k, v in datasets.items():\n",
    "  print(f\"{k} dataset contains {len(v['inputs'])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f65500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf \n",
    "# Unless this import is here, the following error comes up\n",
    "# Error Op type not registered 'CaseFoldUTF8' in binary running on 932fd13e3432. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\n",
    "# You may be trying to load on a different device from the computational device. Consider setting the `experimental_io_device` option in `tf.saved_model.LoadOptions` to the io_device such as '/job:localhost'.\n",
    "import tensorflow_text\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def download_model() -> tf.keras.Model:\n",
    "  preprocessor = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "  encoder = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/2\",\n",
    "    trainable=False)\n",
    "\n",
    "  return preprocessor, encoder\n",
    "\n",
    "preprocessor, encoder = download_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "\n",
    "encoder_inputs = preprocessor(text_input)\n",
    "\n",
    "encoder_outputs = encoder(encoder_inputs)\n",
    "\n",
    "pooled_output = encoder_outputs[\"pooled_output\"]      # [batch_size, 512].\n",
    "#sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 512].\n",
    "hidden_layer = tf.keras.layers.Dense(256, activation=\"gelu\")\n",
    "classif_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "hidden_out = hidden_layer(pooled_output)\n",
    "final_out = classif_layer(hidden_out)\n",
    "\n",
    "model = tf.keras.Model(inputs=text_input, outputs=final_out)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), metrics=\"accuracy\")\n",
    "\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
